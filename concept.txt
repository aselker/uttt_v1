How to represent moves for MCTS and for NN?  Only some are valid.
	* Within cxc, some spots already gone.  Maybe just lose if you pick one?
	* You can only pick within one cxc for early game, eventually you can choose.  Two separate 3x3's, or choose from a full ixi and lose if you pick the wrong one?  Either way, 81 choices, most usually lose.
	* In MCTS, I can easily list all possible moves.  But because it's a variable number, it can't really be the output of a NN.  And I'd rather have the output of the MCTS be the same as from the NN.
	* But, a random playout is likely to hit a suicide move if it's given 81 choices.  So most playouts will be useless, and there might be a ton of positive weight on getting to choose your cxc, because then most moves don't lose.
	* How does AlphaGo do it?
	* Pass out two 3x3's, or a 3x3x3x3, and instantly lose if invalid.  Hard-code into MCTS not to insta-lose, let the NN learn that (it should learn fast).  As an alternative, we could ignore the chosen cxc if we're not free to choose one; this is sort of inconsistent with losing if we choose an occupied spot within the cxc, but might make it easier for the NN to learn to select a cxc.
	* I think it'd be nice to represent a distribution over all possible moves, so there should be 81 values passed out.
	* I think I should do this differently for NN and MCTS.  Eventually, the NN will be sort of part of the MCTS, I think, it'll never run by itself.
	* If the NN only rates how good a position is, vs. suggesting states, I think this is a non-issue.  We just never ask if how good an invalid state is.  ("value network", as opposed to "policy network")

NN structure concepts:
	* AlphaZero has networks both suggest moves and predict how a game state is going.  Should I do that?
		* You can pick a move by "trying" every move and seeing how the estimated states are.  Downsides: ~81x slower, and there might kind of be less signal.  E.g. if you're about to lose no matter what, there's little difference in state between moves.  Which could be fine I guess.
	* Concept 0:
		* A single network, from ixi to {single output or ixi}.  Every layer is fully-connected.
	* Concept 1:
		* 9 small networks, each coming from a cxc.  All weights are shared among the 9, and also the first few layers of each are symmetric (rotation and mirroring).  This is supposed to analyze each cxc, e.g. which moves are good.  Output is probably like 12 neurons per cxc.
		* One large network, which takes inputs from the 9 smaller ones, and outputs {a single output or an ixi}.  Like the smaller ones, some weights are shared across rotation and mirroring.
	* Concept 1.5: Same as Concept 1, but no weights are shared by rotation or mirroring.

How to train?
	* Option 0: Start out with normal self-play, as incompetent as it is.
	* Option 1: Start out with ixi's filled randomly with X's and O's, so they're only a few moves from ending.  Maybe use a few heuristics to make ixi's that are sane: 
		* Same number of X's and O's, or one more of whichever symbol you're not about to play
		* Avoid cxc's where both people won
	* Using MCTS...
		* Just train the NN to give the same answer as MCTS.
		* Play two MCTS bots against each other, train the NN to predict the outcome.

How good is MCTS?
	* To test: play a bot that searches few playouts against one that searches a lot of playouts.

Resources
	* AlphaZero architecture diagram: https://miro.medium.com/max/4000/1*0pn33bETjYOimWjlqDLLNw.png
